{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 6.1 - Keras for RNN\n",
    "\n",
    "In this lab we will use the [Keras deep learning library](https://keras.io/) to construct a simple recurrent neural network (RNN) that can *learn* linguistic structure from a piece of text, and use that knowledge to *generate* new text passages. To review general RNN architecture, specific types of RNN networks such as the LSTM networks we'll be using here, and other concepts behind this type of machine learning, you should consult the following resources:\n",
    "\n",
    "- http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- http://ml4a.github.io/guides/recurrent_neural_networks/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "This code is an adaptation of these two examples:\n",
    "\n",
    "- http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "- https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "You can consult the original sites for more information and documentation.\n",
    "\n",
    "Let's start by importing some of the libraries we'll be using in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is generate our training data set. In this case we will use a recent article written by Barack Obama for The Economist newspaper. Make sure you have the `obama.txt` file in the `/data` folder within the `/week-6` folder in your repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 18312\n",
      "text preview: wherever i go these days, at home or abroad, people ask me the same question: what is happening in the american political system? how has a country that has benefitedperhaps more than any otherfrom immigration, trade and technological innovation suddenly developed a strain of anti-immigrant, anti-innovation protectionism? why have some on the far left and even more on the far right embraced a crude populism that promises a return to a past that is not possible to restoreand that, for most americ\n"
     ]
    }
   ],
   "source": [
    "# load ascii text from file\n",
    "filename = \"data/obama.txt\"\n",
    "raw_text = open(filename).read()\n",
    "\n",
    "# get rid of any characters other than letters, numbers, \n",
    "# and a few special characters\n",
    "raw_text = re.sub('[^\\nA-Za-z0-9 ,.:;?!-]+', '', raw_text)\n",
    "\n",
    "# convert all text to lowercase\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "print \"length of text:\", n_chars\n",
    "print \"text preview:\", raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use python's `set()` function to generate a list of all unique characters in the text. This will form our 'vocabulary' of characters, which is similar to the categories found in typical ML classification problems. \n",
    "\n",
    "Since neural networks work with numerical data, we also need to create a mapping between each character and a unique integer value. To do this we create two dictionaries: one which has characters as keys and the associated integers as the value, and one which has integers as keys and the associated characters as the value. These dictionaries will allow us to do translation both ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique characters found: 44\n",
      "a - maps to -> 18\n",
      "25 - maps to -> h\n"
     ]
    }
   ],
   "source": [
    "# extract all unique characters in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "n_vocab = len(chars)\n",
    "print \"number of unique characters found:\", n_vocab\n",
    "\n",
    "# create mapping of characters to integers and back\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# test our mapping\n",
    "print 'a', \"- maps to ->\", char_to_int[\"a\"]\n",
    "print 25, \"- maps to ->\", int_to_char[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to define the training data for our network. With RNN's, the training data usually takes the shape of a three-dimensional matrix, with the size of each dimension representing:\n",
    "\n",
    "[# of training sequences, # of training samples per sequence, # of features per sample]\n",
    "\n",
    "- The training sequences are the sets of data subjected to the RNN at each training step. As with all neural networks, these training sequences are presented to the network in small batches during training.\n",
    "- Each training sequence is composed of some number of training samples. The number of samples in each sequence dictates how far back in the data stream the algorithm will learn, and sets the depth of the RNN layer.\n",
    "- Each training sample within a sequence is composed of some number of features. This is the data that the RNN layer is learning from at each time step. In our example, the training samples and targets will use one-hot encoding, so will have a feature for each possible character, with the actual character represented by `1`, and all others by `0`.\n",
    "\n",
    "To prepare the data, we first set the length of training sequences we want to use. In this case we will set the sequence length to 100, meaning the RNN layer will be able to predict future characters based on the 100 characters that came before.\n",
    "\n",
    "We will then slide this 100 character 'window' over the entire text to create `input` and `output` arrays. Each entry in the `input` array contains 100 characters from the text, and each entry in the `output` array contains the single character that came after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  18212\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    inputs.append(raw_text[i:i + seq_length])\n",
    "    outputs.append(raw_text[i + seq_length])\n",
    "    \n",
    "n_sequences = len(inputs)\n",
    "print \"Total sequences: \", n_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's shuffle both the input and output data so that we can later have Keras split it automatically into a training and test set. To make sure the two lists are shuffled the same way (maintaining correspondance between inputs and outputs), we create a separate shuffled list of indeces, and use these indeces to reorder both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = range(len(inputs))\n",
    "random.shuffle(indeces)\n",
    "\n",
    "inputs = [inputs[x] for x in indeces]\n",
    "outputs = [outputs[x] for x in indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize one of these sequences to make sure we are getting what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "own by 11 see chart 4. progress in america also helped catalyse the historic paris climate agreement --> ,\n"
     ]
    }
   ],
   "source": [
    "print inputs[0], \"-->\", outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will prepare the actual numpy datasets which will be used to train our network. We first initialize two empty numpy arrays in the proper formatting:\n",
    "\n",
    "- X --> [# of training sequences, # of training samples, # of features]\n",
    "- y --> [# of training sequences, # of features]\n",
    "\n",
    "We then iterate over the arrays we generated in the previous step and fill the numpy arrays with the proper data. Since all character data is formatted using one-hot encoding, we initialize both data sets with zeros. As we iterate over the data, we use the `char_to_int` dictionary to map each character to its related position integer, and use that position to change the related value in the data set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dims --> (18212, 100, 44)\n",
      "y dims --> (18212, 44)\n"
     ]
    }
   ],
   "source": [
    "# create two empty numpy array with the proper dimensions\n",
    "X = np.zeros((n_sequences, seq_length, n_vocab), dtype=np.bool)\n",
    "y = np.zeros((n_sequences, n_vocab), dtype=np.bool)\n",
    "\n",
    "# iterate over the data and build up the X and y data sets\n",
    "# by setting the appropriate indices to 1 in each one-hot vector\n",
    "for i, example in enumerate(inputs):\n",
    "    for t, char in enumerate(example):\n",
    "        X[i, t, char_to_int[char]] = 1\n",
    "    y[i, char_to_int[outputs[i]]] = 1\n",
    "    \n",
    "print 'X dims -->', X.shape\n",
    "print 'y dims -->', y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our RNN model in Keras. This is very similar to how we defined the CNN model, except now we use the `LSTM()` function to create an LSTM layer with an internal memory of 128 neurons. LSTM is a special type of RNN layer which solves the unstable gradients issue seen in basic RNN. Along with LSTM layers, Keras also supports basic RNN layers and GRU layers, which are similar to LSTM. You can find full documentation for recurrent layers in [Keras' documentation](https://keras.io/layers/recurrent/)\n",
    "\n",
    "As before, we need to explicitly define the input shape for the first layer. Also, we need to tell Keras whether the LSTM layer should pass its sequence of predictions or its internal memory as the output to the next layer. If you are connecting the LSTM layer to a fully connected layer as we do in this case, you should set the `return_sequences` parameter to `False` to have the layer pass the value of its hidden neurons. If you are connecting multiple LSTM layers, you should set the parameter to `True` in all but the last layer, so that subsequent layers can learn from the sequence of predictions of previous layers.\n",
    "\n",
    "We will use dropout with a probability of 50% to regularize the network and prevent overfitting on our training data. The output of the network will be a fully connected layer with one neuron for each character in the vocabulary. The softmax function will convert this output to a probability distribution across all characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define two helper functions: one to select a character based on a probability distribution, and one to generate a sequence of predicted characters based on an input (or 'seed') list of characters.\n",
    "\n",
    "The `sample()` function will take in a probability distribution generated by the `softmax()` function, and select a character based on the 'temperature' input. The temperature (also often called the 'diversity') effects how strictly the probability distribution is sampled. \n",
    "\n",
    "- Lower values (closer to zero) output more confident predictions, but are also more conservative. In our case, if the model has overfit the training data, lower values are likely to give back exactly what is found in the text\n",
    "- Higher values (1 and above) introduce more diversity and randomness into the results. This can lead the model to generate novel information not found in the training data. However, you are also likely to see more errors such as grammatical or spelling mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generate()` function will take in:\n",
    "\n",
    "- input sentance ('seed')\n",
    "- number of characters to generate\n",
    "- and target diversity or temperature\n",
    "\n",
    "and print the resulting sequence of characters to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(sentence, prediction_length=50, diversity=0.35):\n",
    "    print '----- diversity:', diversity \n",
    "\n",
    "    generated = sentence\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    # iterate over number of characters requested\n",
    "    for i in range(prediction_length):\n",
    "        \n",
    "        # build up sequence data from current sentence\n",
    "        x = np.zeros((1, X.shape[1], X.shape[2]))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_to_int[char]] = 1.\n",
    "\n",
    "        # use trained model to return probability distribution\n",
    "        # for next character based on input sequence\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # use sample() function to sample next character \n",
    "        # based on probability distribution and desired diversity\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # convert integer to character\n",
    "        next_char = int_to_char[next_index]\n",
    "\n",
    "        # add new character to generated text\n",
    "        generated += next_char\n",
    "        \n",
    "        # delete the first character from beginning of sentance, \n",
    "        # and add new caracter to the end. This will form the \n",
    "        # input sequence for the next predicted character.\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        # print results to screen\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a system for Keras to save our model's parameters to a local file after each epoch where it achieves an improvement in the overall loss. This will allow us to reuse the trained model at a later time without having to retrain it from scratch. This is useful for recovering models incase your computer crashes, or you want to stop the training early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"-basic_LSTM.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready to train the model. We want to train the model over 50 epochs, but we also want to output some generated text after each epoch to see how our model is doing. \n",
    "\n",
    "To do this we create our own loop to iterate over each epoch. Within the loop we first train the model for one epoch. Since all parameters are stored within the model, training one epoch at a time has the same exact effect as training over a longer series of epochs. We also use the model's `validation_split` parameter to tell Keras to automatically split the data into 80% training data and 20% test data for validation. Remember to always shuffle your data if you will be using validation!\n",
    "\n",
    "After each epoch is trained, we use the `raw_text` data to extract a new sequence of 100 characters as the 'seed' for our generated text. Finally, we use our `generate()` helper function to generate text using two different diversity settings.\n",
    "\n",
    "*Warning:* because of their large depth (remember that an RNN trained on a 100 long sequence effectively has 100 layers!), these networks typically take a much longer time to train than traditional multi-layer ANN's and CNN's. You shoud expect these models to train overnight on the virtual machine, but you should be able to see enough progress after the first few epochs to know if it is worth it to train a model to the end. For more complex RNN models with larger data sets in your own work, you should consider a native installation, along with a dedicated GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 3.2191 - val_loss: 2.9699\n",
      "----- generating with seed: ng inequality have resulted in slower income growth for low- and middle-income families. globalisati\n",
      "----- diversity: 0.5\n",
      "ng inequality have resulted in slower income growth for low- and middle-income families. globalisatih e ec ta  o ei nwnui no .n   o  i e  e e oss t o   abeetnoo nene   ig nonetooeet   eninn e s  mreen\n",
      "----- diversity: 1.2\n",
      "ng inequality have resulted in slower income growth for low- and middle-income families. globalisatii8\n",
      "z fyevl ,te:terhifps ovhuartad gs3iefrc oescbrq :wugmse xeet me\n",
      "srpi2js35dx gab8mfmdcialbuo mbf2i\n",
      "epoch: 2 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 44s - loss: 3.0172 - val_loss: 2.9138\n",
      "----- generating with seed: has been decades in the making. while i am proud of what my administration has accomplished these pa\n",
      "----- diversity: 0.5\n",
      "has been decades in the making. while i am proud of what my administration has accomplished these pa ten a tdep  oo ntcoos eonn p t ic a im rceea t ie i os no  dee p th e r te r sese aoe tt ete  eo ot\n",
      "----- diversity: 1.2\n",
      "has been decades in the making. while i am proud of what my administration has accomplished these pahyiteosalpth taidoudalipn ff ddescuadrar 0t o.eee gl  ,t  ehun ;hsebdua?de .ob ris 4l meh rm hwettph\n",
      "epoch: 3 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.9451 - val_loss: 2.8435\n",
      "----- generating with seed: ers were constrained by a greater degree of social interaction between employees at all levelsat chu\n",
      "----- diversity: 0.5\n",
      "ers were constrained by a greater degree of social interaction between employees at all levelsat chue  ldeo trad om ocdte t aone  a et  r  o e  eo ue ha em eo nnem e rr t iers s e s rg p  eotii i nt n\n",
      "----- diversity: 1.2\n",
      "ers were constrained by a greater degree of social interaction between employees at all levelsat chu mgjaido3e .. naewnl ratecbae btimseieii.l trgartyhdtim pfgs imnxgoorur.ctt,9a\n",
      "a\n",
      "sjanim ub   spa iqf\n",
      "epoch: 4 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.8493 - val_loss: 2.7266\n",
      "----- generating with seed: e a powerful force for the common good, driving businesses to create products that consumers rave ab\n",
      "----- diversity: 0.5\n",
      "e a powerful force for the common good, driving businesses to create products that consumers rave ab anoale in tos ater apal o rneb t ocste ser thntah ane eond ne  rofet  teat to tlisian ou tereos  ae\n",
      "----- diversity: 1.2\n",
      "e a powerful force for the common good, driving businesses to create products that consumers rave abvatstd ra.fcnr-sylg gtl.4b sxrtiebhofi vh; hasimg a,rdb a7si eswho dhig tteo thgaehekanuasals inhn h\n",
      "epoch: 5 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.7334 - val_loss: 2.6122\n",
      "----- generating with seed: anges enacted during my administration have increased the share of income received by all other fami\n",
      "----- diversity: 0.5\n",
      "anges enacted during my administration have increased the share of income received by all other famins tat de anthis an  rters to tnr wanrs ine th tr al fn ereaw l tne th as nhe rane ndc  nes ore ahe \n",
      "----- diversity: 1.2\n",
      "anges enacted during my administration have increased the share of income received by all other famikgwiu ingt tthal cuosc tnngw\n",
      "8ebohua ut-draklssird ot imyrs van v2\n",
      "lrglstse foolct he mhe dstuss iwh\n",
      "epoch: 6 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 44s - loss: 2.6374 - val_loss: 2.5356\n",
      "----- generating with seed: nancial institutions accountable, so their customers get loans they can repay with clear terms up-fr\n",
      "----- diversity: 0.5\n",
      "nancial institutions accountable, so their customers get loans they can repay with clear terms up-fret an dother sore the  horud aur auld sppore an whon tor an onas on toretas nalod torter the far the\n",
      "----- diversity: 1.2\n",
      "nancial institutions accountable, so their customers get loans they can repay with clear terms up-freh ursetkexslcrofites .emtath polotll almirine deandg7 bo gufeo pve dsewsem.ereepwolpfrub beut on fb\n",
      "epoch: 7 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 2.5678 - val_loss: 2.4814\n",
      "----- generating with seed: humanity controls as much wealth as the other 99 will never be stable. gaps between rich and poor ar\n",
      "----- diversity: 0.5\n",
      "humanity controls as much wealth as the other 99 will never be stable. gaps between rich and poor ar  the the the winulit tho that tre  ore tre ins an the pony tales ing wose pat an sord theit maw the\n",
      "----- diversity: 1.2\n",
      "humanity controls as much wealth as the other 99 will never be stable. gaps between rich and poor arseumin. stmiin, detta yon yethassteinomerib.ravp?o?y vnot ites imongblitte ettntaciimck. ph sqpimist\n",
      "epoch: 8 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 2.5195 - val_loss: 2.4329\n",
      "----- generating with seed: y-sector emissions by 6, even as our economy has grown by 11 see chart 4. progress in america also h\n",
      "----- diversity: 0.5\n",
      "y-sector emissions by 6, even as our economy has grown by 11 see chart 4. progress in america also he the the ot end gee on mores on th ar cans eo te the the the thin the sosl the fore nle the are the\n",
      "----- diversity: 1.2\n",
      "y-sector emissions by 6, even as our economy has grown by 11 see chart 4. progress in america also h eec?ot ansa olsenx sun hg f,leindi finy brssent won ceguat oi ensastg.abe 3meree enno-solayes 3 p s\n",
      "epoch: 9 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 2.4736 - val_loss: 2.3995\n",
      "----- generating with seed: nations because we are convinced that with hard work, we can improve our own station and watch our c\n",
      "----- diversity: 0.5\n",
      "nations because we are convinced that with hard work, we can improve our own station and watch our cond ferrins the the ins on tarl ats and the fhol  ureston ineat otore she the ond eresite the the as\n",
      "----- diversity: 1.2\n",
      "nations because we are convinced that with hard work, we can improve our own station and watch our c5oisenad itlouthe afiylnfale m.tgo\n",
      "ullatyau,s bnn tiwucsdercremicisuucbagad foson apurhpigme hes bog\n",
      "epoch: 10 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 45s - loss: 2.4413 - val_loss: 2.3673\n",
      "----- generating with seed: rules, but denying that progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do mor\n",
      "----- diversity: 0.5\n",
      "rules, but denying that progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do more the withe sores an foree woro art ald forign ins we in en the the the ante, ald pre the the ant in\n",
      "----- diversity: 1.2\n",
      "rules, but denying that progress leaves us more vulnerable, not less so.\n",
      "\n",
      "america should also do mors bitianme. irvapekns de6 voce tien pogthogsupaf ane fuac;hirgesto pestewhin thean evcvons tasd urs \n",
      "epoch: 11 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 2.4034 - val_loss: 2.3351\n",
      "----- generating with seed: est productivity growth in the g7, but it has slowed across nearly all advanced economies see chart \n",
      "----- diversity: 0.5\n",
      "est productivity growth in the g7, but it has slowed across nearly all advanced economies see chart pate the ale mante fratins tha the the rones and merinte pates in ale comsere muol rore ande thi anv\n",
      "----- diversity: 1.2\n",
      "est productivity growth in the g7, but it has slowed across nearly all advanced economies see chart pimnnmssog8wer aumnpepgyoa ge9,;iclehanl? am-bevccyeehe yongs\n",
      "rhof ag turecivisest poo5;, lima th w \n",
      "epoch: 12 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 45s - loss: 2.3745 - val_loss: 2.3033\n",
      "----- generating with seed: dime and the auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than ev\n",
      "----- diversity: 0.5\n",
      "dime and the auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than eveale ins the the ball greste the deres in whes the toen to for the in prenthe the tien and the maid \n",
      "----- diversity: 1.2\n",
      "dime and the auto industry rescued. i enacted a larger and more front-loaded fiscal stimulus than evende puas the un-oly tirins, spei-ee, bnet tesdiln armecashw nuctite, my goe bhe. apate ddereaw faed\n",
      "epoch: 13 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 45s - loss: 2.3488 - val_loss: 2.2847\n",
      "----- generating with seed:  sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiment in the late 19t\n",
      "----- diversity: 0.5\n",
      " sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiment in the late 19th pisthe enouncon ing as ove ore in sore the ans and the the the thon thal the the bat in meris or t\n",
      "----- diversity: 1.2\n",
      " sedition acts of 1798, the know-nothings of the mid-1800s, the anti-asian sentiment in the late 19ti, joxhddecigndusslaotkanr, facal adl7, letsalim. frote ther rady woconican org wure marp ortht p te\n",
      "epoch: 14 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 45s - loss: 2.3224 - val_loss: 2.2625\n",
      "----- generating with seed: just got some group or idea that was threatening america under control. we overcame those fears and \n",
      "----- diversity: 0.5\n",
      "just got some group or idea that was threatening america under control. we overcame those fears and on the wor wess the fort ond the prontitis or 1re serobit fint ous tor weres rote ant one meor ont s\n",
      "----- diversity: 1.2\n",
      "just got some group or idea that was threatening america under control. we overcame those fears and athe ingrit-ry wdelidithols to 2tunlr. ser thor .noyi nblites ficprpene es lctioe  hoduthe lgtersess\n",
      "epoch: 15 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 2.2964 - val_loss: 2.2429\n",
      "----- generating with seed: eptive to the argument that the game is rigged. but amid this understandable frustration, much of it\n",
      "----- diversity: 0.5\n",
      "eptive to the argument that the game is rigged. but amid this understandable frustration, much of it hat und in the in ine soring the secincan whe recons to eas the sonon growe. in incouling the and b\n",
      "----- diversity: 1.2\n",
      "eptive to the argument that the game is rigged. but amid this understandable frustration, much of ith  99de thipes of therd\n",
      "acs oase vocingerime goud,eheis roficomitt may asmearimlet. ceo 1alcayc, 2r \n",
      "epoch: 16 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.2750 - val_loss: 2.2203\n",
      "----- generating with seed: ss of the past eight years should also give the world some measure of hope. despite all manner of di\n",
      "----- diversity: 0.5\n",
      "ss of the past eight years should also give the world some measure of hope. despite all manner of dine and boolle meras on the tial ons hor the tor and potile for portis an the won lod lort the the fo\n",
      "----- diversity: 1.2\n",
      "ss of the past eight years should also give the world some measure of hope. despite all manner of ditiping, pinvrald carn byea2estgoos voreimess.\n",
      "acs folc8 onicl crovitt nstto-i aln52\n",
      "2?ag ten me6xr a\n",
      "epoch: 17 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.2513 - val_loss: 2.2132\n",
      "----- generating with seed: a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamism\n",
      "\n",
      "first, in recent y\n",
      "----- diversity: 0.5\n",
      "a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamism\n",
      "\n",
      "first, in recent y and the mure and the pare and aning alle minans the progre and the tored ond ont inering res more a\n",
      "----- diversity: 1.2\n",
      "a resilient economy thats primed for future growth.\n",
      "\n",
      "restoring economic dynamism\n",
      "\n",
      "first, in recent yeusus eetharcat. annont coien th amesre th apref the0tidunaqiomacfrndone. ant fwerl tho botlerin abn\n",
      "epoch: 18 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.2277 - val_loss: 2.1952\n",
      "----- generating with seed: s for high-income households, preventing colleges from pricing out hardworking students, and ensurin\n",
      "----- diversity: 0.5\n",
      "s for high-income households, preventing colleges from pricing out hardworking students, and ensuring the non the tus the home ro nent in prestaring to pare the fors the mers gron the porteres tha ind\n",
      "----- diversity: 1.2\n",
      "s for high-income households, preventing colleges from pricing out hardworking students, and ensurinon, nan no ty py\n",
      "-slpltebrwas ciosemite onwthe; grang wmouc inserum ot all oh pront? nod nst weurle \n",
      "epoch: 19 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.2111 - val_loss: 2.1770\n",
      "----- generating with seed: get rich with everybody else. thats the problem with increased inequalityit diminishes upward mobili\n",
      "----- diversity: 0.5\n",
      "get rich with everybody else. thats the problem with increased inequalityit diminishes upward mobiliss to are tho reate for the mons earo the that the ot more paritisat poretan in the the fer for geth\n",
      "----- diversity: 1.2\n",
      "get rich with everybody else. thats the problem with increased inequalityit diminishes upward mobilitu on uribulaib beppakint  f3r wheld 7brebucidening totheies:at onobe at. imem padettob?fcroroult th\n",
      "epoch: 20 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 44s - loss: 2.1779 - val_loss: 2.1652\n",
      "----- generating with seed: aising the federal minimum wage, expanding the earned income tax credit for workers without dependen\n",
      "----- diversity: 0.5\n",
      "aising the federal minimum wage, expanding the earned income tax credit for workers without dependens the the soun fer the thea the tre samerig cans and the ront bes of eres in for the prowored the th\n",
      "----- diversity: 1.2\n",
      "aising the federal minimum wage, expanding the earned income tax credit for workers without dependengy tw bly theearhs move r catteb-eokusd, the llinawtilast pindeate meo lo buat poole thurud in toun,\n",
      "epoch: 21 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.1688 - val_loss: 2.1573\n",
      "----- generating with seed: dozen bills provided 1.4 trillion in economic support from 2009 to 2012but fighting congress for eac\n",
      "----- diversity: 0.5\n",
      "dozen bills provided 1.4 trillion in economic support from 2009 to 2012but fighting congress for eacing tho the poformeres the bithe poesto that and and and inconming the than and that for beamer inca\n",
      "----- diversity: 1.2\n",
      "dozen bills provided 1.4 trillion in economic support from 2009 to 2012but fighting congress for eace inge-ratex.\n",
      "antoud mome groblot m rasesurican gholke in buearion cavey cylcadprinames itol? mmistr\n",
      "epoch: 22 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.1415 - val_loss: 2.1418\n",
      "----- generating with seed: th insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by n\n",
      "----- diversity: 0.5\n",
      "th insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by ne wese song wer the for the ins and te and the that han suling the ind ald in the the more the solte\n",
      "----- diversity: 1.2\n",
      "th insurance, while health-care costs grow at the slowest rate in 50 years; annual deficits cut by nesiwinnmme. aneg.rwel to laki7s, whesfuntutids\n",
      "tingrtadiin-iimcsinco prozeg eforksmywed of al math i\n",
      "epoch: 23 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.1251 - val_loss: 2.1231\n",
      "----- generating with seed: ore vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepare for negative shocks before they\n",
      "----- diversity: 0.5\n",
      "ore vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepare for negative shocks before they to the work at that aver oo sesd an in in the bits the whe dome the pratice the surees antere dican\n",
      "----- diversity: 1.2\n",
      "ore vulnerable, not less so.\n",
      "\n",
      "america should also do more to prepare for negative shocks before they 19akdisx chadj misgr anhohgrepormeting one mha feab-eed owe camleclonsm pergal anskubian ineqla7iin\n",
      "epoch: 24 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 1257s - loss: 2.1056 - val_loss: 2.1171\n",
      "----- generating with seed: is discontent is driven by fears that are not fundamentally economic. the anti-immigrant, anti-mexic\n",
      "----- diversity: 0.5\n",
      "is discontent is driven by fears that are not fundamentally economic. the anti-immigrant, anti-mexicand in and wich ond to the wor the sece of the and and hate ho sefst the the protes on the loalle so\n",
      "----- diversity: 1.2\n",
      "is discontent is driven by fears that are not fundamentally economic. the anti-immigrant, anti-mexicaisl-ost and a5d cvesto biil thing-wusimy efurklg ble pboriles 2tnptpise of chaole, mocy mitity fera\n",
      "epoch: 25 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.0884 - val_loss: 2.1132\n",
      "----- generating with seed: d without costing taxpayers a dime and the auto industry rescued. i enacted a larger and more front-\n",
      "----- diversity: 0.5\n",
      "d without costing taxpayers a dime and the auto industry rescued. i enacted a larger and more front-ince the proted in oul and in the bit an whed bestrex and loches to the wurker the peveris noveres t\n",
      "----- diversity: 1.2\n",
      "d without costing taxpayers a dime and the auto industry rescued. i enacted a larger and more front-uelby, batial ans prareo hin mexily n ee ahexorweld the olad.\n",
      "yed and.-ib tpelllyidy te waup atciasi\n",
      "epoch: 26 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.0722 - val_loss: 2.1039\n",
      "----- generating with seed: productivity, inequality has risen in most advanced economies, with that increase most pronounced in\n",
      "----- diversity: 0.5\n",
      "productivity, inequality has risen in most advanced economies, with that increase most pronounced in the ture and pertican enengres conmere thon the for ancouling the burthen prorte of recaner and con\n",
      "----- diversity: 1.2\n",
      "productivity, inequality has risen in most advanced economies, with that increase most pronounced inimescedima, moal thon ncinupes 19e 20he1dtn4 meriags butd fxemung tqukin 1hd bypysen s: thee raken t\n",
      "epoch: 27 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.0462 - val_loss: 2.0932\n",
      "----- generating with seed: ll street have made our financial system more stable and supportive of long-term growth, including m\n",
      "----- diversity: 0.5\n",
      "ll street have made our financial system more stable and supportive of long-term growth, including moul there hand is of the the fom the pronting to the ated ing out on the enof sud the the lost on me\n",
      "----- diversity: 1.2\n",
      "ll street have made our financial system more stable and supportive of long-term growth, including mosefratedagri-ilat hys funre-hivrte, alyglicy wdeherjois 5nl, nes inlmenng gest-ouly the uconeali.;h\n",
      "epoch: 28 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.0305 - val_loss: 2.0836\n",
      "----- generating with seed: a toll on life satisfaction, self-esteem, physical health and mortality. it is related to a devastat\n",
      "----- diversity: 0.5\n",
      "a toll on life satisfaction, self-esteem, physical health and mortality. it is related to a devastatien that the work fort orlome the portins in the sone for antinisitions and whene moning thaul hace \n",
      "----- diversity: 1.2\n",
      "a toll on life satisfaction, self-esteem, physical health and mortality. it is related to a devastativi t  theseelan rheandngmentmacy gut luve betadticic lloswerdsmigients-alad, wd ecroattrects the 0y\n",
      "epoch: 29 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 2.0142 - val_loss: 2.0783\n",
      "----- generating with seed: gress to pass the trans-pacific partnership and to conclude a transatlantic trade and investment par\n",
      "----- diversity: 0.5\n",
      "gress to pass the trans-pacific partnership and to conclude a transatlantic trade and investment pare encanies mant and enlonging for in prowice in the easte the for alles be too in the ars ane te nou\n",
      "----- diversity: 1.2\n",
      "gress to pass the trans-pacific partnership and to conclude a transatlantic trade and investment pare tor.\n",
      "\n",
      "aneanty chot pmtre and whxrexenss en meren yrowisesmyctete mane rofveluli-an shevuills. that\n",
      "epoch: 30 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 1.9919 - val_loss: 2.0764\n",
      "----- generating with seed: left to their own devices, can fail. this can happen through the tendency towards monopoly and rent-\n",
      "----- diversity: 0.5\n",
      "left to their own devices, can fail. this can happen through the tendency towards monopoly and rent-tising s prove the ald parteris in ome and ecconsemes art oun ore partisen to the incand thes forker\n",
      "----- diversity: 1.2\n",
      "left to their own devices, can fail. this can happen through the tendency towards monopoly and rent-onge, \n",
      "o,rocs maricuvelngbtt, ursertel. the bespcenive. oves ann deus mouice sec meol es oundels, gr\n",
      "epoch: 31 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 44s - loss: 1.9734 - val_loss: 2.0628\n",
      "----- generating with seed: ericas economy is an enormously complicated mechanism. as appealing as some more radical reforms can\n",
      "----- diversity: 0.5\n",
      "ericas economy is an enormously complicated mechanism. as appealing as some more radical reforms can oul desint cant it so wer at re in for the the partion sore who then fin male. the rower domering a\n",
      "----- diversity: 1.2\n",
      "ericas economy is an enormously complicated mechanism. as appealing as some more radical reforms can steco itore toaaly in mosteinenm wo e7es. mifad ig oll, muntovirisgun ace iansing 2udoslod beder of\n",
      "epoch: 32 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 1.9591 - val_loss: 2.0660\n",
      "----- generating with seed: ser to its highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires \n",
      "----- diversity: 0.5\n",
      "ser to its highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires in recasernde the beringesing to best eaprical of thing that in nestroming werkast co the fer wer be\n",
      "----- diversity: 1.2\n",
      "ser to its highest aspirations. so where does my successor go from here?\n",
      "\n",
      "further progress requires tferekwockejs the tuntteb a llrech co ofasu se nag e0buet maidituale in chicpiabig eoro d2btlis the \n",
      "epoch: 33 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 1.9437 - val_loss: 2.0623\n",
      "----- generating with seed: ts of the shadow banking system still present vulnerabilities and the housing-finance system has not\n",
      "----- diversity: 0.5\n",
      "ts of the shadow banking system still present vulnerabilities and the housing-finance system has not enonomy that the prowed sherce the progres proves and the for mear ic and be the for the insertis t\n",
      "----- diversity: 1.2\n",
      "ts of the shadow banking system still present vulnerabilities and the housing-finance system has not carrings vererenh, sotyes wounhan laye pricimes ager grasu freclasests andamibimesa moon crond iver\n",
      "epoch: 34 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 4424s - loss: 1.9268 - val_loss: 2.0546\n",
      "----- generating with seed:  among non-college-educated americansthe group where labour-force participation has fallen most prec\n",
      "----- diversity: 0.5\n",
      " among non-college-educated americansthe group where labour-force participation has fallen most preconis, the prodect ant chal thain and coure for ameriss roweces conded to the sustore for inenges man\n",
      "----- diversity: 1.2\n",
      " among non-college-educated americansthe group where labour-force participation has fallen most preciyns  now-chrase ann vem jotex 2010.. meca. isow on mwil wstotinn oncgurcr aftr enrf\n",
      "amitizy tad onr\n",
      "epoch: 35 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 44s - loss: 1.9016 - val_loss: 2.0548\n",
      "----- generating with seed: ministration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need to be \n",
      "----- diversity: 0.5\n",
      "ministration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need to be ther the portrens that or the devering the woct om the f prover for the surit the part our enone, th\n",
      "----- diversity: 1.2\n",
      "ministration since at least 1960.\n",
      "\n",
      "even these efforts fall well short. in the future, we need to be inoiss, pead les?minc antirg deftoinst. 1bes it thery and busisniteso. regerat and aln g1otit brogit\n",
      "epoch: 36 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 1.8842 - val_loss: 2.0421\n",
      "----- generating with seed: d a pervasive sense of injustice undermines peoples faith in the system. without trust, capitalism a\n",
      "----- diversity: 0.5\n",
      "d a pervasive sense of injustice undermines peoples faith in the system. without trust, capitalism ald bution growth and anding the instaring for ane prostint the butt in be alb enon the in progress a\n",
      "----- diversity: 1.2\n",
      "d a pervasive sense of injustice undermines peoples faith in the system. without trust, capitalism and gublintotg peth1ye 5o 9ual. hicad haveenof by ally alporruthom gho , shecid fon th evurmepdt.\n",
      "qtu\n",
      "epoch: 37 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 1.8646 - val_loss: 2.0473\n",
      "----- generating with seed: he top see chart 2. under my administration, we will have boosted incomes for families in the bottom\n",
      "----- diversity: 0.5\n",
      "he top see chart 2. under my administration, we will have boosted incomes for families in the bottom erofurict on the past geer mane my ance of the preses for the pald to the work for eronon axportand\n",
      "----- diversity: 1.2\n",
      "he top see chart 2. under my administration, we will have boosted incomes for families in the bottom myecrored ystrencataimel fysecs cakrecany mase mal anle sontment-bove beg taed us ov fowt are fon f\n",
      "epoch: 38 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 43s - loss: 1.8545 - val_loss: 2.0468\n",
      "----- generating with seed: to dangerous products or overly expensive health insurance.\n",
      "\n",
      "more fundamentally, a capitalism shaped\n",
      "----- diversity: 0.5\n",
      "to dangerous products or overly expensive health insurance.\n",
      "\n",
      "more fundamentally, a capitalism shaped and in butreat dentsing to the lease showt dane in rase mone the growe for erenon stuthe that shere\n",
      "----- diversity: 1.2\n",
      "to dangerous products or overly expensive health insurance.\n",
      "\n",
      "more fundamentally, a capitalism shaped nom vervunof,aincal, biad jovite lnowe moraveetshind alxgredise in 2qwirl o50 io moeus oo pans pigr\n",
      "epoch: 39 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 3108s - loss: 1.8368 - val_loss: 2.0550\n",
      "----- generating with seed: ese innovations have changed lives, they have not yet substantially boosted measured productivity gr\n",
      "----- diversity: 0.5\n",
      "ese innovations have changed lives, they have not yet substantially boosted measured productivity growth in the werd actiontits ard on the prousting an we fore for elpestitisis that proven stowe thas \n",
      "----- diversity: 1.2\n",
      "ese innovations have changed lives, they have not yet substantially boosted measured productivity gromdimont. mons toe ba tot, ul on wegudiditive wo. teence to of to rewlivilses iverobuts nc, mepreg o\n",
      "epoch: 40 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.8244 - val_loss: 2.0467\n",
      "----- generating with seed: y with clear terms up-front.\n",
      "\n",
      "but even with all the progress, segments of the shadow banking system \n",
      "----- diversity: 0.5\n",
      "y with clear terms up-front.\n",
      "\n",
      "but even with all the progress, segments of the shadow banking system the soop the the promore for the prosters and wechist than the parkure shouthrat then the farturitin\n",
      "----- diversity: 1.2\n",
      "y with clear terms up-front.\n",
      "\n",
      "but even with all the progress, segments of the shadow banking system thain.; avernekta bes tien tee poogtrcs and allter\n",
      "\n",
      "ymentmsiclassins as rait babilefds tue.\n",
      "\n",
      "ias, bu\n",
      "epoch: 41 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.8007 - val_loss: 2.0433\n",
      "----- generating with seed: e internet, mobile broadband and devices, artificial intelligence, robotics, advanced materials, imp\n",
      "----- diversity: 0.5\n",
      "e internet, mobile broadband and devices, artificial intelligence, robotics, advanced materials, improst dimentt, that partingstest and ander can be to in and goo the and income the proftring the and \n",
      "----- diversity: 1.2\n",
      "e internet, mobile broadband and devices, artificial intelligence, robotics, advanced materials, imploesis. tat ilice smoment geores of equar inp robled aro owrmirndigilishal omoricrias the 1ys gop hu\n",
      "epoch: 42 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 47s - loss: 1.7817 - val_loss: 2.0535\n",
      "----- generating with seed: g poverty, and the beginnings of a reversal in inequality; 20m more americans with health insurance,\n",
      "----- diversity: 0.5\n",
      "g poverty, and the beginnings of a reversal in inequality; 20m more americans with health insurance, the ton mest then growth prowing ane thit poreste the for in the lase the wepresting the and fich a\n",
      "----- diversity: 1.2\n",
      "g poverty, and the beginnings of a reversal in inequality; 20m more americans with health insurance, fut aingtexdendysicm mase pay. 2kef 251.\n",
      "\n",
      "efses y,er\n",
      " iononcen biyer comibititive on ous expoy prod\n",
      "epoch: 43 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.7688 - val_loss: 2.0503\n",
      "----- generating with seed: . as appealing as some more radical reforms can sound in the abstractbreaking up all the biggest ban\n",
      "----- diversity: 0.5\n",
      ". as appealing as some more radical reforms can sound in the abstractbreaking up all the biggest bans the paly in out the portere and fur and the probitisifing tou eed eate for asturion and the ald me\n",
      "----- diversity: 1.2\n",
      ". as appealing as some more radical reforms can sound in the abstractbreaking up all the biggest bans in butir tarex ircost yuldeves for prestredy  ang onortadaim, bliles cane thor condmm? bufterndet \n",
      "epoch: 44 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.7475 - val_loss: 2.0539\n",
      "----- generating with seed: s acknowledged that the work of perfecting our union would take far longer. the presidency is a rela\n",
      "----- diversity: 0.5\n",
      "s acknowledged that the work of perfecting our union would take far longer. the presidency is a relant pootred and amicisst fas ee love tha the ald on the to paring the ond canting to bakd recenting t\n",
      "----- diversity: 1.2\n",
      "s acknowledged that the work of perfecting our union would take far longer. the presidency is a relarb on s: atey. n sp ind be cane be d ofces akl,s am romitx abiliaess urcuaby foundifing derece sunde\n",
      "epoch: 45 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.7282 - val_loss: 2.0498\n",
      "----- generating with seed: t has slowed across nearly all advanced economies see chart 1. without a faster-growing economy, we \n",
      "----- diversity: 0.5\n",
      "t has slowed across nearly all advanced economies see chart 1. without a faster-growing economy, we lean theal encome the to tor fur the porestidis ad the ture of the to burt to in besiges to the tary\n",
      "----- diversity: 1.2\n",
      "t has slowed across nearly all advanced economies see chart 1. without a faster-growing economy, we bun denlyeecs, insewerd cark of mriitn im auritipressprisget hos in s. ew. by adp hate of a dezans, \n",
      "epoch: 46 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.7230 - val_loss: 2.0456\n",
      "----- generating with seed: made real progress on all these fronts. but i believe that changes in culture and values have also p\n",
      "----- diversity: 0.5\n",
      "made real progress on all these fronts. but i believe that changes in culture and values have also pecen the cont mere priste to the efonome the woth of the enfort fur cong to ur of the lage in reeasi\n",
      "----- diversity: 1.2\n",
      "made real progress on all these fronts. but i believe that changes in culture and values have also pechdes, ad seuthequel chiteche sce al seaktteing, incede cscenteyn sicrmesicisucaed ries wove chubcc\n",
      "epoch: 47 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.6969 - val_loss: 2.0688\n",
      "----- generating with seed: x reform that lowers statutory rates and closes loopholes, and with public investments in basic rese\n",
      "----- diversity: 0.5\n",
      "x reform that lowers statutory rates and closes loopholes, and with public investments in basic resents shaled and workeng the poonty and worke of the rives wat pall on in the pat by that economy that\n",
      "----- diversity: 1.2\n",
      "x reform that lowers statutory rates and closes loopholes, and with public investments in basic resesta ing forl ad yaem 20hut o5 the abnam siplost be bblen sefpre utiprinly agd mugh rhard thainw dhas\n",
      "epoch: 48 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.6836 - val_loss: 2.0663\n",
      "----- generating with seed: banks or erecting prohibitively steep tariffs on importsthe economy is not an abstraction. it cannot\n",
      "----- diversity: 0.5\n",
      "banks or erecting prohibitively steep tariffs on importsthe economy is not an abstraction. it cannot ir allimat onermation in the pasticising thaus addices in the to prove thet in and the resenty come\n",
      "----- diversity: 1.2\n",
      "banks or erecting prohibitively steep tariffs on importsthe economy is not an abstraction. it cannotu comfuldsy aspre3sse to dhirs coppest poryeco thet menssuning wriver somuneacse, tag by fibted inqu\n",
      "epoch: 49 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.6705 - val_loss: 2.0619\n",
      "----- generating with seed: ng my administration have increased the share of income received by all other families by more than \n",
      "----- diversity: 0.5\n",
      "ng my administration have increased the share of income received by all other families by more than the income the proare so at and sumality and butinisit and incenty in the reaver can ou thest an the\n",
      "----- diversity: 1.2\n",
      "ng my administration have increased the share of income received by all other families by more than the growoncmest fer ane 4c alnoctisizang robyeb. no oo ho biavifns enverese, mace doulthresare imadi\n",
      "epoch: 50 / 50\n",
      "Train on 14569 samples, validate on 3643 samples\n",
      "Epoch 1/1\n",
      "14569/14569 [==============================] - 46s - loss: 1.6467 - val_loss: 2.0728\n",
      "----- generating with seed: r, it is important to remember that capitalism has been the greatest driver of prosperity and opport\n",
      "----- diversity: 0.5\n",
      "r, it is important to remember that capitalism has been the greatest driver of prosperity and opportsan to in the par pation the provest ane ges parte and the progress for and of enonomily ate the pas\n",
      "----- diversity: 1.2\n",
      "r, it is important to remember that capitalism has been the greatest driver of prosperity and opportivisisl, res-ejanicho mhate-ictongthe joptiemy thas fincneelys ccael agrers marn pmelylacig sejstt b\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "prediction_length = 100\n",
    "\n",
    "for iteration in range(epochs):\n",
    "    \n",
    "    print 'epoch:', iteration + 1, '/', epochs\n",
    "    model.fit(X, y, validation_split=0.2, batch_size=256, nb_epoch=1, callbacks=callbacks_list)\n",
    "    \n",
    "    # get random starting point for seed\n",
    "    start_index = random.randint(0, len(raw_text) - seq_length - 1)\n",
    "    # extract seed sequence from raw text\n",
    "    seed = raw_text[start_index: start_index + seq_length]\n",
    "    \n",
    "    print '----- generating with seed:', seed\n",
    "    \n",
    "    for diversity in [0.5, 1.2]:\n",
    "        generate(seed, prediction_length, diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! You can see that the RNN has learned alot of the linguistic structure of the original writing, including typical length for words, where to put spaces, and basic punctuation with commas and periods. Many words are still misspelled but seem almost reasonable, and it is pretty amazing that it is able to learn this much in only 50 epochs of training. \n",
    "\n",
    "You can see that the loss is still going down after 50 epochs, so the model can definitely benefit from longer training. If you're curious you can try to train for more epochs, but as the error decreases be careful to monitor the output to make sure that the model is not overfitting. As with other neural network models, you can monitor the difference between training and validation loss to see if overfitting might be occuring. In this case, since we're using the model to generate new information, we can also get a sense of overfitting from the material it generates.\n",
    "\n",
    "A good indication of overfitting is if the model outputs exactly what is in the original text given a seed from the text, but jibberish if given a seed that is not in the original text. Remember we don't want the model to learn how to reproduce exactly the original text, but to learn its style to be able to generate new text. As with other models, regularization methods such as dropout and limiting model complexity can be used to avoid the problem of overfitting.\n",
    "\n",
    "Finally, let's save our training data and character to integer mapping dictionaries to an external file so we can reuse it with the model at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to -basic_data.pickle\n",
      "Compressed pickle size: 80934860\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '-basic_data.pickle'\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'int_to_char': int_to_char,\n",
    "        'char_to_int': char_to_int,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print 'Unable to save data to', pickle_file, ':', e\n",
    "    raise\n",
    "    \n",
    "statinfo = os.stat(pickle_file)\n",
    "print 'Saved data to', pickle_file\n",
    "print 'Compressed pickle size:', statinfo.st_size"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
